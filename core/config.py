# ---------------------- llama.cpp server endpoint ----------------------

# This is where llama-server.exe is listening
# /completion is the default llama.cpp text generation endpoint
LLAMA_SERVER_URL = "http://127.0.0.1:8081/completion"
